# Copy to configs/train.yaml and edit values.
model_name: "unsloth/Llama-3.2-3B-Instruct"
dataset_path: "data/train.jsonl"
output_dir: "output"
max_seq_length: 2048
load_in_4bit: true
use_gradient_checkpointing: "unsloth"

# LoRA
lora_r: 16
lora_alpha: 16
lora_dropout: 0.0
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
bias: "none"

# Training
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
warmup_steps: 5
max_steps: 60
learning_rate: 0.0002
logging_steps: 1
optim: "adamw_8bit"
weight_decay: 0.01
lr_scheduler_type: "linear"
seed: 42

# Save
save_steps: 30
save_total_limit: 2
